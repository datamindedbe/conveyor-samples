Conveyor and SQLMesh's Write-Audit-Publish pattern
==================================================
 
SQLMesh's _virtual data environments_ are an example of the
_Write-Audit-Publish_ (WAP) pattern: by creating _views_ in non-production
schemas to physical tables managed by SQLMesh, one could audit these relations
prior to publishing them to the production schema.

With Conveyor, usually one has a production environment and one or more
non-production environments. These environments are essentially Kubernetes
namespaces, with a dedicated Airflow service per namespace for scheduling
purposes.

In classical setups with Conveyor, one would have the codebase use different
storage locations for the non-prod and prod data based on some parameters that
can be sourced from the Conveyor environment. That is up to the coder though to
make this distinction. Indeed, the coder could reuse the same data catalog in
dev and prod, which would not grant him/her the freedom to experiment with
changes to the codebase that would effect the data only in dev. 

As far as Conveyor is concerned, it doesn't care about the semantics of the
data, or where it is stored. That means that a SQLMesh codebase can
conveniently be run in a single Conveyor environment. For development, one
would then branch off from the main branch (which is assumed to be the one that
the last `conveyor project deploy` was run from), make the desired changes to
models and macros, and run `sqlmesh plan some_env`. New tables _may_ be created
as a result, which can have some impact on the data warehouse in terms of load.
These new tables can be audited and if found okay, the changes can be merged
into the main branch, on which `conveyor project deploy` would run again.
Tobiko, the creators of SQLMesh, even provide a [CICD plugin](https://www.tobikodata.com/blog/intro-sqlmesh-cicd-bot).
The architecture for this setup would look like this:

![How Conveyor makes use of SQLMesh's virtual data environments](./static/conveyor_and_virtual_data_envs.png)

Note that only a single warehouse (gateway, in SQLMesh's terms) is used. This allows people to make good use of the virtual data environments. In terms of code, one would add the following to the Airflow operator of Conveyor:

```python
ConveyorContainerOperatorV2(
    env_vars={
        "VIRTUAL_DATA_ENV": "" if os.environ["CONVEYOR_ENV"] == "PROD" else os.environ["CONVEYOR_ENV"],
            },
)
```

and your OCI image would pick that up:

```
# Dockerfile
...
CMD sqlmesh plan --auto-apply ${VIRTUAL_DATA_ENV}
```

Note that one may choose to `--skip-tests` and `--skip-linter` and have
those executed as part of a rigorous CI pipeline to save some time.

There are many scenarios though where such a setup is prohibited, e.g. in
organizations where developers are not allowed to access production data. In
such cases, one cannot make good use of SQLMesh's virtual data environments and
one would instead fall back to a conventional "publish changes in one
environment, audit, then publish in another environment".

![A setup in which Conveyor does not make use of SQLMesh's virtual environments](./static/conveyor_without_virtual_data_envs.png)


## Testing

```bash
sqlmesh create_test sqlmesh_example.lineitem_summary_enriched --query sqlmesh_example.lineitem_summary "SELECT * FROM sqlmesh_example.lineitem_summary LIMIT 3"
```

## Testing in SQLMesh

SQLMesh incorporates two primary testing mechanisms: Audits for data quality validation and Unit Tests for verifying model logic.

### Audits (Data Quality Validation)

Audits evaluate the data generated by models, analogous to dbt tests. They can enforce constraints like uniqueness, non-nullability, or implement custom data quality rules.

**Key Aspects:**

*   **Definition:** Audits are defined inline within model files using the `AUDIT` keyword or as standalone, reusable SQL files (e.g., [`all_positive_values`](../audits/all_positive_values.sql)). Common built-in audits include `not_null` and `unique_key`.
*   **Execution Trigger:** Audits are evaluated during `sqlmesh plan` *only* for models undergoing breaking changes. Audits on unmodified models or newly added audits are not evaluated during `plan`. All relevant audits are executed during `sqlmesh run` or when invoked directly via `sqlmesh audit`.
*   **Failure Modes:** Audits can be configured as blocking or non-blocking.
    *   **Blocking:** Failure prevents view updates in virtual environments and halts `sqlmesh run`.
    *   **Non-blocking:** Failure generates a warning, but view updates and `sqlmesh run` proceed.

We provide an example custom audit at [`all_positive_values`](../audits/all_positive_values.sql) can be applied to a specific column (e.g., `total_amount`) within a model's `MODEL` configuration block under the `audits` key.

### Unit Tests 

Unit tests validate a model's transformation logic using mock input data and asserting expected output, enabling testing independent of the data warehouse.

**Key Aspects:**

*   **Structure:** Defined in YAML files (see [`test_lineitem_summary_enriched.yaml`](../tests/test_lineitem_summary_enriched.yaml)), specifying input data fixtures for the model and its dependencies (`GIVEN` clauses) and the expected output (`THEN` clause).
*   **Test Generation:** SQLMesh has a really helpful command, `sqlmesh create_test`, that can build the 
basic test file for you.
*   **Execution:** Run tests via the `sqlmesh test` command.

**Creating a Unit Test:**

To get started with a unit test, you can run a command like this:

```bash
# Example: Generate a test for lineitem_summary_enriched,
# using 3 rows from lineitem_summary as input fixture
sqlmesh create_test sqlmesh_example.lineitem_summary_enriched \
  --query sqlmesh_example.lineitem_summary "SELECT * FROM sqlmesh_example.lineitem_summary LIMIT 3"
```

This command creates a YAML file like [`test_lineitem_summary_enriched.yaml`](../tests/test_lineitem_summary_enriched.yaml). It sets up the structure and fills in the `inputs` section based on the query you provided.
