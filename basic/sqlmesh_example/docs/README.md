Conveyor and SQLMesh's Write-Audit-Publish pattern
==================================================
 
SQLMesh's _virtual data environments_ are an example of the
_Write-Audit-Publish_ (WAP) pattern: by creating _views_ in non-production
schemas to physical tables managed by SQLMesh, one could audit these relations
prior to publishing them to the production schema.

With Conveyor, usually one has a production environment and one or more
non-production environments. These environments are essentially Kubernetes
namespaces, with a dedicated Airflow service per namespace for scheduling
purposes.

In classical setups with Conveyor, one would have the codebase use different
storage locations for the non-prod and prod data based on some parameters that
can be sourced from the Conveyor environment. That is up to the coder though to
make this distinction. Indeed, the coder could reuse the same data catalog in
dev and prod, which would not grant him/her the freedom to experiment with
changes to the codebase that would effect the data only in dev. 

As far as Conveyor is concerned, it doesn't care about the semantics of the
data, or where it is stored. That means that a SQLMesh codebase can
conveniently be run in a single Conveyor environment. For development, one
would then branch off from the main branch (which is assumed to be the one that
the last `conveyor project deploy` was run from), make the desired changes to
models and macros, and run `sqlmesh plan some_env`. New tables _may_ be created
as a result, which can have some impact on the data warehouse in terms of load.
These new tables can be audited and if found okay, the changes can be merged
into the main branch, on which `conveyor project deploy` would run again.
Tobiko, the creators of SQLMesh, even provide a [CICD plugin](https://www.tobikodata.com/blog/intro-sqlmesh-cicd-bot).
The architecture for this setup would look like this:

![How Conveyor makes use of SQLMesh's virtual data environments](./static/conveyor_and_virtual_data_envs.png)

Note that only a single warehouse (gateway, in SQLMesh's terms) is used. This allows people to make good use of the virtual data environments. In terms of code, one would add the following to the Airflow operator of Conveyor:

```python
ConveyorContainerOperatorV2(
    env_vars={
        "VIRTUAL_DATA_ENV": "" if os.environ["CONVEYOR_ENV"] == "PROD" else os.environ["CONVEYOR_ENV"],
            },
)
```

and your OCI image would pick that up:

```
# Dockerfile
...
CMD sqlmesh plan --auto-apply ${VIRTUAL_DATA_ENV}
```

Note that one may choose to `--skip-tests` and `--skip-linter` and have
those executed as part of a rigorous CI pipeline to save some time.

There are many scenarios though where such a setup is prohibited, e.g. in
organizations where developers are not allowed to access production data. In
such cases, one cannot make good use of SQLMesh's virtual data environments and
one would instead fall back to a conventional "publish changes in one
environment, audit, then publish in another environment".

![A setup in which Conveyor does not make use of SQLMesh's virtual environments](./static/conveyor_without_virtual_data_envs.png)

## Testing in SQLMesh

SQLMesh incorporates two primary testing mechanisms: Audits for data quality validation and Unit Tests for verifying model logic.

### Audits (Data Quality Validation)

Audits evaluate the data generated by models, analogous to dbt tests. They can enforce constraints like uniqueness, non-nullability, or implement custom data quality rules.

**Key Aspects:**

*   **Definition:** Audits are defined inline within model files using the `AUDIT` keyword or as standalone, reusable SQL files (e.g., [`all_positive_values`](../audits/all_positive_values.sql)). Common built-in audits include `not_null` and `unique_key`.
*   **Execution Trigger:** Audits are allways evaluated during `sqlmesh plan` All relevant audits are executed during `sqlmesh run` or when invoked directly via `sqlmesh audit`.
*   **Failure Modes:** Audits can be configured as blocking or non-blocking.
    *   **Blocking:** Failure prevents view updates in virtual environments and halts `sqlmesh run`.
    *   **Non-blocking:** Failure generates a warning, but view updates and `sqlmesh run` proceed.

We provide an example custom audit at [`all_positive_values`](../audits/all_positive_values.sql) can be applied to a specific column (e.g., `total_amount`) within a model's `MODEL` configuration block under the `audits` key.

### Unit Tests 

Unit tests validate a model's transformation logic using mock input data and asserting expected output, enabling testing independent of the data warehouse.

**Key Aspects:**

*   **Structure:** Defined in YAML files (see [`test_lineitem_summary_enriched.yaml`](../tests/test_lineitem_summary_enriched.yaml)), specifying input data fixtures for the model and its dependencies (`GIVEN` clauses) and the expected output (`THEN` clause).
*   **Test Generation:** SQLMesh has a really helpful command, `sqlmesh create_test`, that can build the 
basic test file for you.
*   **Execution:** Run tests via the `sqlmesh test` command.

**Creating a Unit Test:**

To get started with a unit test, you can run a command like this:

```bash
# Example: Generate a test for lineitem_summary_enriched,
# using 3 rows from lineitem_summary as input fixture
sqlmesh create_test sqlmesh_example.lineitem_summary_enriched \
  --query sqlmesh_example.lineitem_summary "SELECT * FROM sqlmesh_example.lineitem_summary LIMIT 3"
```

This command creates a YAML file like [`test_lineitem_summary_enriched.yaml`](../tests/test_lineitem_summary_enriched.yaml). It sets up the structure and fills in the `inputs` section based on the query you provided.

## Is the WAP pattern from sqlmesh easily aligned with the way of working using conveyor?
When we dig into implementing a WAP pattern where **each step (Write, Audit, Publish) is distinctly separate and easy for developers to debug**, we encounter some challenges with SQLMesh's current approach. Here's a breakdown of why achieving that specific WAP implementation feels tricky:

**1. Integrated Workflows: plan vs. run**

SQLMesh doesn't offer distinct commands like sqlmesh write, sqlmesh audit, and sqlmesh publish. Instead, its core commands, sqlmesh plan and sqlmesh run, bundle these stages internally. Usually the development workflow is as follows:
    1. Make local changes
    2. Run ```sqlmesh plan``` on your dev environment
    3. Promote changes to prod by doing ```sqlmesh plan prod```
    4. To process new incoming data with the current version of the model schedule calls to ```sqlmesh run prod```
    5. Iterate and repeat

Here, it is imporant to note the differences between the workflows of plan and run as they have implications for the WAP question:

- **sqlmesh plan:**
    - **Workflow:** When you run plan, SQLMesh compares your local code changes against the state of the target environment. It figures out what models are affected. For these models, it might:
        - **Test**: Run unit tests to check the correct logic of the current code. If the tests fail, the next steps are cancelled.
        - **Write:** Create *new physical tables* (snapshots) representing the *new version* of your model's logic. This is especially true for full models or significantly changed incremental models requiring a restatement.
        - **Audit:** Run the audits defined for those models against the data in these *newly created snapshot tables*.
        - **Publish:** If the audits pass (and other checks are okay), SQLMesh promotes these new snapshots, making them the "official" version in that environment (often by updating views).
- **sqlmesh run:** This command is primarily about **processing new data intervals for existing, promoted models**
    - **Workflow:** run looks for time intervals that haven't been processed yet according to the model's definition *already promoted in the environment*. It doesn't use your local code changes.
        - **Write (Internal):** For an incremental model and a new data slice, it typically computes *just that slice*, often storing it in a temporary table or structure.
        - **Audit (Internal):** It runs audits specifically against *this newly computed data slice* in its temporary location.
        - **Publish (Internal Merge):** If the audit on the slice passes, it merges this new data into the *existing* physical snapshot table for that model. The snapshot ID itself doesn't usually change, the table just gets updated. For full models, run usually doesn't do much unless they are configured to refresh on intervals.

**The Challenge:** In order to hide complexity, sqmlesh makes opaque their internal Write, Audit, and Publish stages and we cannot access the intermediate tables in case of failures in a clear way. The process is designed to be more atomic.

**2. Audit Control Doesn't Facilitate a Staged WAP**

You want to potentially Write data, inspect it, *then* run Audits, and *then* Publish. SQLMesh's audit handling makes this specific flow difficult:

- **Default Behavior:** Failing audits halt plan application or run processing. This is good for safety but prevents the "Write, then Audit later" approach. One question is trying to skip the audits in dev, write first and then run the audits. However, this is not possible, we can only skip the unit tests when we run either plan or run.

Essentially, there isn't a built-in way to say, "Okay, SQLMesh, create the new table data (Write), but hold off on promoting it. Now, let me explicitly trigger the audits (Audit). If they pass, *then* I'll give the command to make it live (Publish)."

**3. It is difficult to debug when things go wrong, Especially for Incremental Failures**

When things go wrong, particularly during the Audit phase, quickly finding the problematic data is key.

- **plan Failures:** If a plan fails during an audit, ```sqlmesh table_info [MODEL] --dev``` is quite helpful. It points you to the specific physical snapshot table created based on your *current code*. You can (with the right permissions) query this table directly to see why the audit failed. This works well because the failure relates to a complete table version generated from your changed code.
- **run Failures:** Debugging audit failures during ```sqlmesh run``` seems trickier, especially for incremental models. The audit fails on the *specific batch of new data* being processed, likely in a temporary location before the merge. The question is can developers easily query *that specific temporary batch*? If the run command fails and potentially cleans up temporary resources, isolating the exact rows causing the audit failure within that specific interval's data becomes challenging. You're left knowing the merge didn't happen, but seeing the "why" in the data might be hard.
- **Access Permissions:** Even when you can identify the table (like a snapshot table from a failed plan), developers need the necessary database permissions (e.g., SELECT on the sqlmesh schema) to query these physical tables directly. The physical tables are written into the internal sqlmesh_project schema using the configured service principal, thus not direcltly accessible to developers.